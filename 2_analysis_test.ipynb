{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4df34ebd-ea60-44fa-a2bf-bef758b518a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\purva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\purva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded reviews.csv. Shape: (10, 2)\n",
      "   rating                                        review_text\n",
      "0     5.0  Very good ac, its new and its cooling is excel...\n",
      "1     5.0  Very good ac, its new and its cooling is excel...\n",
      "2     5.0  Very good ac, its new and its cooling is excel...\n",
      "3     1.0  Delivered on time and installation was done sa...\n",
      "4     4.0  Even though I purchased the product online the...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# --- NLTK setup (run once) ---\n",
    "#nltk.download('stopwords') # Downloads the list of \"stopwords\" (a, the, is, etc.)\n",
    "#nltk.download('punkt') # Downloads the tokenizer model\n",
    "# -----------------------------\n",
    "\n",
    "# Load the spaCy model we downloaded\n",
    "# We disable parts we don't need (parser, ner) to make it faster\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Load our reviews from Phase 1\n",
    "try:\n",
    "    df = pd.read_csv('reviews.csv')\n",
    "    print(f\"Successfully loaded reviews.csv. Shape: {df.shape}\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: reviews.csv not found. Did you complete Phase 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6253ce65-f890-4a8e-9381-89118832c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with sentiment scores:\n",
      "   rating                                        review_text  sentiment\n",
      "0     5.0  Very good ac, its new and its cooling is excel...     0.9689\n",
      "1     5.0  Very good ac, its new and its cooling is excel...     0.9689\n",
      "2     5.0  Very good ac, its new and its cooling is excel...     0.9689\n",
      "3     1.0  Delivered on time and installation was done sa...     0.2617\n",
      "4     4.0  Even though I purchased the product online the...     0.8351\n",
      "\n",
      "--- Most Positive Review ---\n",
      "Very satisfied with Daikin. Build quality is excellent. Services of AC Planet, installation agency are also outstanding. Mr. Souvik who installed the AC is very efficient. Thanks to Daikin, AC planet and Amazon team. All are requested to extend their best services to all customers in future also.Read more\n",
      "\n",
      "--- Most Negative Review ---\n",
      "Where do I even begin?I rarely write reviews, but after the nightmare I’ve had with Daikin, I feel it’s my responsibility to warn others. And to make things worse — my previous review mysteriously disappeared from Amazon. Not sure if it was removed, but it’s incredibly suspicious.1. Missing Essential Components:The AC was delivered without basic parts like the copper pipe. How do you ship an AC unit without something so fundamental? This delayed the entire installation process unnecessarily.2. Clueless Installation Team:When the installation team finally came, they were unprepared — no core cutting tools. Even worse, Daikin does not allow installation by third-party professionals, so you’re stuck with their unprofessional service. They had to leave installation in between and left the house in mess. Pic attached.3. No Customer Service:I’ve written multiple complaints over the last month. Not a single response. Zero accountability. It's like once they’ve sold the unit, they just vanish.4. Hidden/Extra Charges:The installation team tried to charge for every little thing e.g. installation of wall brackets. Why am I even paying a separate “installation fee” if everything is extra?5. Review Censorship?:My earlier, honest review is no longer visible. That alone says a lot about how Daikin handles feedback.I expected professionalism from a premium brand. Instead, I got delays, poor service, zero support, and extra charges. If you're considering Daikin — don’t. Save yourself the stress and go with a brand that actually values its customers.Read more\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the VADER analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 2. Define a function to get the sentiment\n",
    "def get_vader_sentiment(text):\n",
    "    # VADER's polarity_scores() returns a dictionary\n",
    "    # We just want the 'compound' score, which is a single number from -1 (v. neg) to +1 (v. pos)\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# 3. Apply this function to every review in our 'review_text' column\n",
    "# and store it in a new 'sentiment' column\n",
    "df['sentiment'] = df['review_text'].apply(get_vader_sentiment)\n",
    "\n",
    "# 4. Check our work\n",
    "print(\"DataFrame with sentiment scores:\")\n",
    "print(df.head())\n",
    "\n",
    "# Let's see the most positive and negative reviews\n",
    "print(\"\\n--- Most Positive Review ---\")\n",
    "print(df.loc[df['sentiment'].idxmax()]['review_text'])\n",
    "\n",
    "print(\"\\n--- Most Negative Review ---\")\n",
    "print(df.loc[df['sentiment'].idxmin()]['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8b0471-52e7-43f9-a47a-8e221be51645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text pre-processing...\n",
      "Pre-processing complete.\n",
      "\n",
      "--- Original vs. Processed ---\n",
      "Original: Very good ac, its new and its cooling is excellent. If you know how to use a AC efficiently, this is a great choice. I set it at 27 degrees, and it us\n",
      "Processed: good ac new cooling excellent know use ac efficiently great choice set degree use rate load hour night hope work like use extra voltage stabiliser bui\n"
     ]
    }
   ],
   "source": [
    "# Get the list of stopwords from NLTK\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# You can add custom stopwords that are common in your reviews but not useful\n",
    "# e.g., 'amazon', 'product', 'review', 'buy', 'bought'\n",
    "custom_stopwords = ['amazon', 'product', 'review', 'buy', 'bought', 'get']\n",
    "stopwords.extend(custom_stopwords)\n",
    "\n",
    "\n",
    "# Define our cleaning and lemmatizing function\n",
    "def preprocess_text(text):\n",
    "    # 1. Create a \"doc\" object using spaCy\n",
    "    doc = nlp(text.lower()) # Lowercase the text\n",
    "    \n",
    "    # 2. Create a list of lemmatized tokens (words)\n",
    "    #    that are not stopwords, not punctuation, and are alphabetic\n",
    "    processed_tokens = [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha\n",
    "    ]\n",
    "    \n",
    "    # 3. Join the tokens back into a single string\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# 4. Apply this function to all reviews\n",
    "#    This can take a minute or two if you have thousands of reviews\n",
    "print(\"Starting text pre-processing...\")\n",
    "df['processed_text'] = df['review_text'].apply(preprocess_text)\n",
    "print(\"Pre-processing complete.\")\n",
    "\n",
    "# 5. Check the difference\n",
    "print(\"\\n--- Original vs. Processed ---\")\n",
    "print(\"Original:\", df['review_text'].iloc[0][:150])\n",
    "print(\"Processed:\", df['processed_text'].iloc[0][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace9b93c-0aba-43fa-a11d-578bae5b53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (10, 154)\n",
      "LDA model fitting complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Vectorize: Convert text into a matrix of word frequencies\n",
    "#    TfidfVectorizer finds words that are important (frequent in one doc, rare in others)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, # Ignore words that appear in > 95% of docs\n",
    "    min_df=2,    # Ignore words that appear in < 2 docs\n",
    "    ngram_range=(1,2) # Consider single words (1,1) and two-word phrases (1,2)\n",
    ")\n",
    "\n",
    "# \"Fit\" the vectorizer to our processed text and transform the text into a matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "\n",
    "# 2. Run LDA (Latent Dirichlet Allocation)\n",
    "#    This is the core topic modeling algorithm\n",
    "num_topics = 5 # Let's look for 5 main topics\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=num_topics, \n",
    "    random_state=42 # For reproducible results\n",
    ")\n",
    "\n",
    "# Fit the LDA model to our matrix\n",
    "lda.fit(tfidf_matrix)\n",
    "print(\"LDA model fitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9699c21-f707-4b93-8cb4-d1e599fd1da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for each topic:\n",
      "\n",
      "--- Topic #0 ---\n",
      "performance, efficient, daikin ac, ac, daikin, product, cool, start, service, good\n",
      "\n",
      "--- Topic #1 ---\n",
      "like, work, excellent, room, good, customer, amazon, thank, swing, brand\n",
      "\n",
      "--- Topic #2 ---\n",
      "like, work, excellent, room, good, customer, amazon, thank, swing, brand\n",
      "\n",
      "--- Topic #3 ---\n",
      "use, installation, fan, set, ac, good, use extra, voltage stabiliser, use rate, voltage\n",
      "\n",
      "--- Topic #4 ---\n",
      "installation, service, daikin, product, support, purchase, charge, job, research, online\n"
     ]
    }
   ],
   "source": [
    "# Helper function to print the topics\n",
    "def print_topics(model, vectorizer, n_top_words):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Get the top words for this topic\n",
    "        top_words = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"\\n--- Topic #{topic_idx} ---\")\n",
    "        print(', '.join(top_words))\n",
    "\n",
    "# Print the top 10 words for each of our 5 topics\n",
    "print(\"Top words for each topic:\")\n",
    "print_topics(model=lda, vectorizer=vectorizer, n_top_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71de5504-3765-4384-ad0f-8f2f466cad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
